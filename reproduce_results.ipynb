{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4d28b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from utils import get_features_from_df, cast_list_as_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05ef402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to train and VALIDATE your solution\n",
    "train_df = pd.read_csv(\"./data/quora_train_data.csv\")\n",
    "\n",
    "# use this to provide the expected generalization results\n",
    "test_df = pd.read_csv(\"./data/quora_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57046887",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train =  cast_list_as_strings(list(train_df[\"question1\"]))\n",
    "q2_train =  cast_list_as_strings(list(train_df[\"question2\"]))\n",
    "q1_test  =  cast_list_as_strings(list(test_df[\"question1\"]))\n",
    "q2_test  =  cast_list_as_strings(list(test_df[\"question2\"]))\n",
    "all_questions = q1_train + q2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2388b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer(ngram_range=(1,1))\n",
    "count_vectorizer.fit(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07084bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_q1q2 = get_features_from_df(train_df,count_vectorizer)\n",
    "X_te_q1q2  = get_features_from_df(test_df, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788320c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[\"is_duplicate\"].values\n",
    "y_test = test_df[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "580f0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Metrics\n",
      "Train Accuracy: 0.7744\n",
      "Train Precision: 0.7106\n",
      "Train Recall: 0.6549\n",
      "Train F1-score: 0.6816\n",
      "Train ROC AUC: 0.8518\n"
     ]
    }
   ],
   "source": [
    "perceptron = joblib.load(\"perceptron_model.joblib\", mmap_mode=None)\n",
    "y_train_pred = perceptron.predict(X_tr_q1q2)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "precision = precision_score(y_train, y_train_pred)\n",
    "recall = recall_score(y_train, y_train_pred)\n",
    "f1 = f1_score(y_train, y_train_pred)\n",
    "roc_auc = roc_auc_score(y_train, perceptron.decision_function(X_tr_q1q2))\n",
    "\n",
    "print(\"Perceptron Metrics Train\")\n",
    "print(\"Train Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Train Precision: {:.4f}\".format(precision))\n",
    "print(\"Train Recall: {:.4f}\".format(recall))\n",
    "print(\"Train F1-score: {:.4f}\".format(f1))\n",
    "print(\"Train ROC AUC: {:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69e62427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Metrics Test\n",
      "Train Accuracy: 0.6962\n",
      "Train Precision: 0.5933\n",
      "Train Recall: 0.5752\n",
      "Train F1-score: 0.5841\n",
      "Train ROC AUC: 0.7493\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = perceptron.predict(X_te_q1q2)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "roc_auc = roc_auc_score(y_test, perceptron.decision_function(X_te_q1q2))\n",
    "\n",
    "print(\"Perceptron Metrics Test\")\n",
    "print(\"Train Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Train Precision: {:.4f}\".format(precision))\n",
    "print(\"Train Recall: {:.4f}\".format(recall))\n",
    "print(\"Train F1-score: {:.4f}\".format(f1))\n",
    "print(\"Train ROC AUC: {:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3045345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics Train\n",
      "Train Accuracy: 0.8119\n",
      "Train Precision: 0.7794\n",
      "Train Recall: 0.6835\n",
      "Train F1-score: 0.7283\n",
      "Train ROC AUC: 0.8877\n"
     ]
    }
   ],
   "source": [
    "logistic = joblib.load(\"logistic_model.joblib\", mmap_mode=None)\n",
    "y_train_pred = logistic.predict(X_tr_q1q2)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_pred)\n",
    "precision = precision_score(y_train, y_train_pred)\n",
    "recall = recall_score(y_train, y_train_pred)\n",
    "f1 = f1_score(y_train, y_train_pred)\n",
    "roc_auc = roc_auc_score(y_train, logistic.predict_proba(X_tr_q1q2)[:, 1])\n",
    "\n",
    "print(\"Logistic Regression Metrics Train\")\n",
    "print(\"Train Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Train Precision: {:.4f}\".format(precision))\n",
    "print(\"Train Recall: {:.4f}\".format(recall))\n",
    "print(\"Train F1-score: {:.4f}\".format(f1))\n",
    "print(\"Train ROC AUC: {:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b824f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics Test\n",
      "Train Accuracy: 0.7536\n",
      "Train Precision: 0.6877\n",
      "Train Recall: 0.6150\n",
      "Train F1-score: 0.6493\n",
      "Train ROC AUC: 0.8119\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = logistic.predict(X_te_q1q2)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "roc_auc = roc_auc_score(y_test, logistic.predict_proba(X_te_q1q2)[:, 1])\n",
    "\n",
    "print(\"Logistic Regression Metrics Test\")\n",
    "print(\"Train Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Train Precision: {:.4f}\".format(precision))\n",
    "print(\"Train Recall: {:.4f}\".format(recall))\n",
    "print(\"Train F1-score: {:.4f}\".format(f1))\n",
    "print(\"Train ROC AUC: {:.4f}\".format(roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
